from pyspark.sql import SparkSession
from pyspark.sql.functions import col, monotonically_increasing_id

# Initialize Spark session
spark = SparkSession.builder \
    .appName("DataFrame Comparison") \
    .getOrCreate()

# Your two DataFrames: df1 and df2
# Assuming they have the same schema and same number of rows

# Add an index to both DataFrames
df1 = df1.withColumn("index", monotonically_increasing_id())
df2 = df2.withColumn("index", monotonically_increasing_id())

# Join the DataFrames on the index
joined_df = df1.join(df2, on="index", how="inner", suffixes=("_df1", "_df2"))

# Create a filter condition for mismatched rows
mismatch_condition = " OR ".join([f"{col_name}_df1 != {col_name}_df2" for col_name in df1.columns if col_name != "index"])

# Filter the joined DataFrame for mismatched rows
mismatched_rows = joined_df.filter(mismatch_condition)

# Remove the index and suffixes from column names
for col_name in df1.columns:
    if col_name != "index":
        mismatched_rows = mismatched_rows.withColumnRenamed(f"{col_name}_df1", col_name)

# Drop unnecessary columns
columns_to_drop = [f"{col_name}_df2" for col_name in df1.columns if col_name != "index"] + ["index"]
mismatched_rows = mismatched_rows.drop(*columns_to_drop)

# Show the report DataFrame with mismatched rows and column names
mismatched_rows.show()
