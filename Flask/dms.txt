from pyspark.sql import SparkSession
from pyspark.sql.functions import col, monotonically_increasing_id, when, array, concat_ws

# Initialize Spark session
spark = SparkSession.builder \
    .appName("DataFrame Comparison") \
    .getOrCreate()

# Your two DataFrames: df1 and df2
# Assuming they have the same schema and same number of rows

# Add an index to both DataFrames
df1 = df1.withColumn("index", monotonically_increasing_id())
df2 = df2.withColumn("index", monotonically_increasing_id())

# Rename columns in df2
df2 = df2.selectExpr([f"{col_name} as {col_name}_df2" if col_name != "index" else "index" for col_name in df2.columns])

# Join the DataFrames on the index
joined_df = df1.join(df2, on="index", how="inner")

# Create an array column containing the names of mismatched columns
mismatched_columns = array(*[
    when(col(col_name) != col(f"{col_name}_df2"), col_name).otherwise(None) for col_name in df1.columns if col_name != "index"
])

# Add the array column to the joined DataFrame
joined_df = joined_df.withColumn("mismatched_columns", mismatched_columns)

# Create a filter condition for rows with mismatched columns
mismatch_condition = when(col("mismatched_columns").isNull(), False).otherwise(True)

# Filter the joined DataFrame for rows with mismatched columns
mismatched_rows = joined_df.filter(mismatch_condition)

# Remove unnecessary columns and index
columns_to_drop = [f"{col_name}_df2" for col_name in df1.columns if col_name != "index"] + ["index"]
mismatched_rows = mismatched_rows.drop(*columns_to_drop)

# Show the report DataFrame with mismatched rows and column names, without truncation
mismatched_rows.show(truncate=False)
