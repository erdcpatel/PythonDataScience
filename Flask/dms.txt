import csv
import json
from confluent_kafka import Consumer, KafkaException

# Create a Consumer instance
consumer_conf = {
    'bootstrap.servers': 'your_bootstrap_servers',
    'group.id': 'your_group_id',
    'auto.offset.reset': 'earliest'
}

consumer = Consumer(consumer_conf)

# Subscribe to your Kafka topics
topics = ['topic1', 'topic2']  # Replace with your actual topics
consumer.subscribe(topics)

# Create a CSV file
csv_file_path = 'output.csv'

try:
    while True:
        # Poll for messages
        msg = consumer.poll(timeout=1000)  # adjust timeout as needed

        if msg is None:
            continue
        if msg.error():
            if msg.error().code() == KafkaException._PARTITION_EOF:
                # End of partition event, not an error
                continue
            else:
                print(f"Error: {msg.error()}")
                break

        # Access message details
        topic = msg.topic()
        partition = msg.partition()
        value = msg.value()

        # Convert value to string (assuming it's in bytes)
        value_str = value.decode('utf-8') if value else None

        # Parse JSON
        data = json.loads(value_str)

        # Open the CSV file in write mode
        with open(csv_file_path, 'a', newline='') as csvfile:
            csv_writer = csv.DictWriter(csvfile, fieldnames=['Topic', 'Partition'] + list(data.keys()))

            # Write header if file is empty
            if csvfile.tell() == 0:
                csv_writer.writeheader()

            # Write data to CSV
            csv_writer.writerow({'Topic': topic, 'Partition': partition, **data})

except KeyboardInterrupt:
    pass

finally:
    # Close down consumer to commit final offsets.
    consumer.close()
